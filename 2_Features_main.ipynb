{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import init_spark\n",
        "import pyspark.sql.functions as sf\n",
        "from pyspark.sql.window import Window as sw\n",
        "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
        "spark = init_spark({\"appName\": 'main_features'})"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "plt.style.use(\"bmh\")\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = spark.table('alfa.andrey_auto_train')\n",
        "test = spark.table('alfa.andrey_auto_test')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make real amnt"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('amnt', sf.round(sf.exp(sf.col('amnt') * sf.lit(17.8209)) - sf.lit(1)))\n",
        "test = test.withColumn('amnt', sf.round(sf.exp(sf.col('amnt') * sf.lit(17.8209)) - sf.lit(1)))\n",
        "\n",
        "train = train.withColumn('log10_amnt', sf.log10('amnt'))\n",
        "test = test.withColumn('log10_amnt', sf.log10('amnt'))\n",
        "\n",
        "train = train.withColumn('n_order', sf.col('log10_amnt').cast(IntegerType()))\n",
        "test = test.withColumn('n_order', sf.col('log10_amnt').cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preproc"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Number order"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_number_order = train.groupBy(['app_id']).agg(sf.mean('log10_amnt').alias('order_number_mean'),\n",
        "                                                   sf.mean('n_order').alias('order_number_int_mean'),\n",
        "                                                   sf.max('log10_amnt').alias('order_number_max'),\n",
        "                                                   sf.max('n_order').alias('order_number_int_max'),\n",
        "                                        sf.expr('percentile_approx(log10_amnt, 0.5)').alias('order_number_median'),\n",
        "                                        sf.expr('percentile_approx(n_order, 0.5)').alias('order_number_int_median'))\n",
        "\n",
        "test_number_order = test.groupBy(['app_id']).agg(sf.mean('log10_amnt').alias('order_number_mean'),\n",
        "                                                 sf.mean('n_order').alias('order_number_int_mean'),\n",
        "                                                 sf.max('log10_amnt').alias('order_number_max'),\n",
        "                                                 sf.max('n_order').alias('order_number_int_max'),\n",
        "                                        sf.expr('percentile_approx(log10_amnt, 0.5)').alias('order_number_median'),\n",
        "                                        sf.expr('percentile_approx(n_order, 0.5)').alias('order_number_int_median'))"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Features with hour_diff"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_hours_diff_features = train.select(['app_id', 'hour_diff'])\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_0',\n",
        "                                                (sf.col('hour_diff') == sf.lit(0)).cast(IntegerType()))\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_1_2',\n",
        "                ((sf.col('hour_diff') >= sf.lit(1)) & (sf.col('hour_diff') <= sf.lit(2))).cast(IntegerType()))\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_3_10',\n",
        "                ((sf.col('hour_diff') >= sf.lit(3)) & (sf.col('hour_diff') <= sf.lit(10))).cast(IntegerType()))\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_1_10',\n",
        "                ((sf.col('hour_diff') >= sf.lit(1)) & (sf.col('hour_diff') <= sf.lit(10))).cast(IntegerType()))\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_11_100',\n",
        "                ((sf.col('hour_diff') >= sf.lit(11)) & (sf.col('hour_diff') <= sf.lit(100))).cast(IntegerType()))\n",
        "train_hours_diff_features = train_hours_diff_features.withColumn('is_100_plus',\n",
        "                (sf.col('hour_diff') > sf.lit(100)).cast(IntegerType()))\n",
        "\n",
        "train_hours_diff_features = train_hours_diff_features.groupBy(['app_id']).agg(\n",
        "                                                            sf.mean('is_0').alias('hour_diff_fraq_0'),\n",
        "                                                            sf.mean('is_1_2').alias('hour_diff_fraq_1_2'),\n",
        "                                                            sf.mean('is_1_10').alias('hour_diff_fraq_1_10'),\n",
        "                                                            sf.mean('is_3_10').alias('hour_diff_fraq_3_10'),\n",
        "                                                            sf.mean('is_11_100').alias('hour_diff_fraq_11_100'),\n",
        "                                                            sf.mean('is_100_plus').alias('hour_diff_fraq_100_plus'))"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_hours_diff_features = test.select(['app_id', 'hour_diff'])\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_0',\n",
        "                                                (sf.col('hour_diff') == sf.lit(0)).cast(IntegerType()))\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_1_2',\n",
        "                ((sf.col('hour_diff') >= sf.lit(1)) & (sf.col('hour_diff') <= sf.lit(2))).cast(IntegerType()))\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_3_10',\n",
        "                ((sf.col('hour_diff') >= sf.lit(3)) & (sf.col('hour_diff') <= sf.lit(10))).cast(IntegerType()))\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_1_10',\n",
        "                ((sf.col('hour_diff') >= sf.lit(1)) & (sf.col('hour_diff') <= sf.lit(10))).cast(IntegerType()))\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_11_100',\n",
        "                ((sf.col('hour_diff') >= sf.lit(11)) & (sf.col('hour_diff') <= sf.lit(100))).cast(IntegerType()))\n",
        "test_hours_diff_features = test_hours_diff_features.withColumn('is_100_plus',\n",
        "                (sf.col('hour_diff') > sf.lit(100)).cast(IntegerType()))\n",
        "\n",
        "test_hours_diff_features = test_hours_diff_features.groupBy(['app_id']).agg(\n",
        "                                                            sf.mean('is_0').alias('hour_diff_fraq_0'),\n",
        "                                                            sf.mean('is_1_2').alias('hour_diff_fraq_1_2'),\n",
        "                                                            sf.mean('is_1_10').alias('hour_diff_fraq_1_10'),\n",
        "                                                            sf.mean('is_3_10').alias('hour_diff_fraq_3_10'),\n",
        "                                                            sf.mean('is_11_100').alias('hour_diff_fraq_11_100'),\n",
        "                                                            sf.mean('is_100_plus').alias('hour_diff_fraq_100_plus'))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Income_flag"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('income_flag', (sf.col('income_flag') == sf.lit(1)).cast(IntegerType()))\n",
        "test = test.withColumn('income_flag', (sf.col('income_flag') == sf.lit(1)).cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_income_flag_table = train.select(['app_id']).dropDuplicates()\n",
        "\n",
        "c_df = train.groupBy(['app_id', 'income_flag']).agg(sf.max('amnt').alias('max_amnt'))\n",
        "c_df = c_df.groupBy(['app_id']).pivot('income_flag').agg(sf.first('max_amnt'))\n",
        "c_df = c_df.withColumnRenamed('0', 'max_debet_income').withColumnRenamed('1', 'max_credit_income')\n",
        "c_df = c_df.fillna(0, subset=['max_debet_income', 'max_credit_income'])\n",
        "c_df = c_df.withColumn('frac_max_debet_credit_income', (sf.col('max_debet_income') + sf.lit(0.001)) /\\\n",
        "                                                       (sf.col('max_credit_income') + sf.lit(0.001)))\n",
        "train_income_flag_table = train_income_flag_table.join(c_df, on=['app_id'], how='left')\n",
        "\n",
        "c_df = train.groupBy(['app_id', 'income_flag']).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "c_df = c_df.groupBy(['app_id']).pivot('income_flag').agg(sf.first('sum_amnt'))\n",
        "c_df = c_df.withColumnRenamed('0', 'sum_debet_income').withColumnRenamed('1', 'sum_credit_income')\n",
        "c_df = c_df.fillna(0, subset=['sum_debet_income', 'sum_credit_income'])\n",
        "c_df = c_df.withColumn('frac_sum_debet_credit_income', (sf.col('sum_debet_income') + sf.lit(0.001)) /\\\n",
        "                                                       (sf.col('sum_credit_income') + sf.lit(0.001)))\n",
        "train_income_flag_table = train_income_flag_table.join(c_df, on=['app_id'], how='left')\n",
        "\n",
        "c_df = train.groupBy(['app_id', 'income_flag', 'days_before']).agg(sf.sum('amnt').alias('sum_amnt'),\n",
        "                                                                   sf.count('amnt').alias('cnt_amnt'))\n",
        "c_df = c_df.withColumn('min_dys_before', sf.min('days_before').over(sw().partitionBy('app_id')))\n",
        "c_df = c_df.withColumn('max_dys_before', sf.max('days_before').over(sw().partitionBy('app_id')))\n",
        "c_df = c_df.withColumn('known_days', sf.col('max_dys_before') - sf.col('min_dys_before'))\n",
        "c_df = c_df.groupBy(['app_id', 'income_flag']).agg(sf.count('sum_amnt').alias('cnt_unique_days_amnt'),\n",
        "                                                   sf.sum('cnt_amnt').alias('cnt_all_trans'),\n",
        "                                                   sf.first('known_days').alias('cnt_all_days'),\n",
        "                                                   \n",
        "                                                   sf.mean('sum_amnt').alias('mean_day_amnt'),\n",
        "                                                   sf.max('sum_amnt').alias('max_day_amnt'),\n",
        "                                                   sf.stddev('sum_amnt').alias('std_day_amnt'),\n",
        "\n",
        "                                                   sf.mean('cnt_amnt').alias('mean_day_cnt_amnt'),\n",
        "                                                   sf.stddev('cnt_amnt').alias('std_day_cnt_amnt'))\n",
        "\n",
        "c_df = c_df.withColumn('fraq_trans_days', sf.col('cnt_unique_days_amnt') / sf.col('cnt_all_days'))\n",
        "c_df = c_df.withColumn('trans_per_month', sf.col('cnt_all_trans') / sf.col('cnt_all_days') * sf.lit(30))\n",
        "c_df = c_df.withColumn('trans_per_day_known', sf.col('cnt_unique_days_amnt') / sf.col('cnt_all_trans'))\n",
        "c_df = c_df.withColumn('zp_days', sf.col('max_day_amnt') / sf.col('trans_per_day_known'))\n",
        "c_df = c_df.withColumn('zp_month', sf.col('trans_per_month') * sf.col('mean_day_amnt'))\n",
        "\n",
        "c_df_0 = c_df.filter(sf.col('income_flag') == 0)\n",
        "select_0_cols = ['mean_day_cnt_amnt', 'mean_day_amnt', 'std_day_amnt', 'fraq_trans_days',\n",
        "                 'trans_per_month', 'zp_days', 'zp_month']\n",
        "c_df_0 = c_df_0.select(['app_id'] + select_0_cols)\n",
        "for col in select_0_cols:\n",
        "    c_df_0 = c_df_0.withColumnRenamed(col, col + '_debet')\n",
        "train_income_flag_table = train_income_flag_table.join(c_df_0, on=['app_id'], how='left')\n",
        "\n",
        "c_df_1 = c_df.filter(sf.col('income_flag') == 1)\n",
        "select_1_cols = ['mean_day_cnt_amnt', 'std_day_cnt_amnt', 'mean_day_amnt', 'std_day_amnt', 'fraq_trans_days',\n",
        "                 'trans_per_month', 'trans_per_day_known']\n",
        "c_df_1 = c_df_1.select(['app_id'] + select_1_cols)\n",
        "for col in select_1_cols:\n",
        "    c_df_1 = c_df_1.withColumnRenamed(col, col + '_credit')\n",
        "train_income_flag_table = train_income_flag_table.join(c_df_1, on=['app_id'], how='left')"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_income_flag_table = test.select(['app_id']).dropDuplicates()\n",
        "\n",
        "c_df = test.groupBy(['app_id', 'income_flag']).agg(sf.max('amnt').alias('max_amnt'))\n",
        "c_df = c_df.groupBy(['app_id']).pivot('income_flag').agg(sf.first('max_amnt'))\n",
        "c_df = c_df.withColumnRenamed('0', 'max_debet_income').withColumnRenamed('1', 'max_credit_income')\n",
        "c_df = c_df.fillna(0, subset=['max_debet_income', 'max_credit_income'])\n",
        "c_df = c_df.withColumn('frac_max_debet_credit_income', (sf.col('max_debet_income') + sf.lit(0.001)) /\\\n",
        "                                                       (sf.col('max_credit_income') + sf.lit(0.001)))\n",
        "test_income_flag_table = test_income_flag_table.join(c_df, on=['app_id'], how='left')\n",
        "\n",
        "c_df = test.groupBy(['app_id', 'income_flag']).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "c_df = c_df.groupBy(['app_id']).pivot('income_flag').agg(sf.first('sum_amnt'))\n",
        "c_df = c_df.withColumnRenamed('0', 'sum_debet_income').withColumnRenamed('1', 'sum_credit_income')\n",
        "c_df = c_df.fillna(0, subset=['sum_debet_income', 'sum_credit_income'])\n",
        "c_df = c_df.withColumn('frac_sum_debet_credit_income', (sf.col('sum_debet_income') + sf.lit(0.001)) /\\\n",
        "                                                       (sf.col('sum_credit_income') + sf.lit(0.001)))\n",
        "test_income_flag_table = test_income_flag_table.join(c_df, on=['app_id'], how='left')\n",
        "\n",
        "c_df = test.groupBy(['app_id', 'income_flag', 'days_before']).agg(sf.sum('amnt').alias('sum_amnt'),\n",
        "                                                                   sf.count('amnt').alias('cnt_amnt'))\n",
        "c_df = c_df.withColumn('min_dys_before', sf.min('days_before').over(sw().partitionBy('app_id')))\n",
        "c_df = c_df.withColumn('max_dys_before', sf.max('days_before').over(sw().partitionBy('app_id')))\n",
        "c_df = c_df.withColumn('known_days', sf.col('max_dys_before') - sf.col('min_dys_before'))\n",
        "c_df = c_df.groupBy(['app_id', 'income_flag']).agg(sf.count('sum_amnt').alias('cnt_unique_days_amnt'),\n",
        "                                                   sf.sum('cnt_amnt').alias('cnt_all_trans'),\n",
        "                                                   sf.first('known_days').alias('cnt_all_days'),\n",
        "                                                   \n",
        "                                                   sf.mean('sum_amnt').alias('mean_day_amnt'),\n",
        "                                                   sf.max('sum_amnt').alias('max_day_amnt'),\n",
        "                                                   sf.stddev('sum_amnt').alias('std_day_amnt'),\n",
        "\n",
        "                                                   sf.mean('cnt_amnt').alias('mean_day_cnt_amnt'),\n",
        "                                                   sf.stddev('cnt_amnt').alias('std_day_cnt_amnt'))\n",
        "\n",
        "c_df = c_df.withColumn('fraq_trans_days', sf.col('cnt_unique_days_amnt') / sf.col('cnt_all_days'))\n",
        "c_df = c_df.withColumn('trans_per_month', sf.col('cnt_all_trans') / sf.col('cnt_all_days') * sf.lit(30))\n",
        "c_df = c_df.withColumn('trans_per_day_known', sf.col('cnt_unique_days_amnt') / sf.col('cnt_all_trans'))\n",
        "c_df = c_df.withColumn('zp_days', sf.col('max_day_amnt') / sf.col('trans_per_day_known'))\n",
        "c_df = c_df.withColumn('zp_month', sf.col('trans_per_month') * sf.col('mean_day_amnt'))\n",
        "\n",
        "c_df_0 = c_df.filter(sf.col('income_flag') == 0)\n",
        "select_0_cols = ['mean_day_cnt_amnt', 'mean_day_amnt', 'std_day_amnt', 'fraq_trans_days',\n",
        "                 'trans_per_month', 'zp_days', 'zp_month']\n",
        "c_df_0 = c_df_0.select(['app_id'] + select_0_cols)\n",
        "for col in select_0_cols:\n",
        "    c_df_0 = c_df_0.withColumnRenamed(col, col + '_debet')\n",
        "test_income_flag_table = test_income_flag_table.join(c_df_0, on=['app_id'], how='left')\n",
        "\n",
        "c_df_1 = c_df.filter(sf.col('income_flag') == 1)\n",
        "select_1_cols = ['mean_day_cnt_amnt', 'std_day_cnt_amnt', 'mean_day_amnt', 'std_day_amnt', 'fraq_trans_days',\n",
        "                 'trans_per_month', 'trans_per_day_known']\n",
        "c_df_1 = c_df_1.select(['app_id'] + select_1_cols)\n",
        "for col in select_1_cols:\n",
        "    c_df_1 = c_df_1.withColumnRenamed(col, col + '_credit')\n",
        "test_income_flag_table = test_income_flag_table.join(c_df_1, on=['app_id'], how='left')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Ecommerce flag"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('ecommerce_flag', (sf.col('ecommerce_flag') == sf.lit(1)).cast(IntegerType()))\n",
        "test = test.withColumn('ecommerce_flag', (sf.col('ecommerce_flag') == sf.lit(1)).cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Currency"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('rub_trans', (sf.col('currency') == sf.lit(1)).cast(IntegerType()))\n",
        "test = test.withColumn('rub_trans', (sf.col('currency') == sf.lit(1)).cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Countries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "country_mapper = {'13': 'bad', '8': 'bad', '16': 'bad', '19': 'bad',\n",
        "                  '1': 'med', '4': 'med', '3': 'med', '2': 'med', '5': 'med', '11': 'med',\n",
        "                  '6': 'good', '7': 'good', '9': 'good', '10': 'good', '12': 'good', '14': 'good', '15': 'good',\n",
        "                  '17': 'good', '18': 'good', '20': 'good', '21': 'good', '22': 'good', '23': 'good', '24': 'good'}\n",
        "iplookup_udf_country = sf.udf(lambda x: country_mapper[x])\n",
        "train = train.withColumn('country_group', iplookup_udf_country(sf.col('country').cast(StringType())))\n",
        "test = test.withColumn('country_group', iplookup_udf_country(sf.col('country').cast(StringType())))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Mcc_category"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mcc_category_mapper = {'28': 'bad', '23': 'bad', '8': 'bad',\n",
        "                       '3': 'med', '12': 'med', '16': 'med', '1': 'med', '2': 'med', '6': 'med', '4': 'med',\n",
        "                       '11': 'med', '5': 'med', '24': 'med', '17': 'med', '14': 'med', '27': 'med', '18': 'med',\n",
        "                       '10': 'good', '7': 'good', '15': 'good', '9': 'good', '13': 'good', '19': 'good',\n",
        "                       '22': 'good', '21': 'good', '20': 'good', '26': 'good', '25': 'good'}\n",
        "iplookup_udf_mcc_category = sf.udf(lambda x: mcc_category_mapper[x])\n",
        "train = train.withColumn('mcc_category_group', iplookup_udf_mcc_category(sf.col('mcc_category').cast(StringType())))\n",
        "test = test.withColumn('mcc_category_group', iplookup_udf_mcc_category(sf.col('mcc_category').cast(StringType())))"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Hour"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('is_night_trans', sf.col('hour').isin({0, 1, 2, 3, 23}).cast(IntegerType()))\n",
        "test = test.withColumn('is_night_trans', sf.col('hour').isin({0, 1, 2, 3, 23}).cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Weekday"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.withColumn('is_weekend_trans_67', sf.col('day_of_week').isin({6, 7}).cast(IntegerType()))\n",
        "train = train.withColumn('is_weekend_trans_12', sf.col('day_of_week').isin({1, 2}).cast(IntegerType()))\n",
        "\n",
        "test = test.withColumn('is_weekend_trans_67', sf.col('day_of_week').isin({6, 7}).cast(IntegerType()))\n",
        "test = test.withColumn('is_weekend_trans_12', sf.col('day_of_week').isin({1, 2}).cast(IntegerType()))"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Age"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_age_table = train.filter(sf.col('weekofyear').isin({8, 10}))\n",
        "train_age_table = train_age_table.groupBy(['app_id', 'weekofyear']).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "train_age_table = train_age_table.groupBy('app_id').pivot('weekofyear').agg(sf.first('sum_amnt'))\n",
        "train_age_table = train_age_table.withColumn('age_frac_8_10_with_null', sf.col(\"8\") / (sf.col(\"8\") + sf.col(\"10\")))\n",
        "train_age_table = train_age_table.select(['app_id', 'age_frac_8_10_with_null'])\n",
        "\n",
        "test_age_table = test.filter(sf.col('weekofyear').isin({8, 10}))\n",
        "test_age_table = test_age_table.groupBy(['app_id', 'weekofyear']).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "test_age_table = test_age_table.groupBy('app_id').pivot('weekofyear').agg(sf.first('sum_amnt'))\n",
        "test_age_table = test_age_table.withColumn('age_frac_8_10_with_null', sf.col(\"8\") / (sf.col(\"8\") + sf.col(\"10\")))\n",
        "test_age_table = test_age_table.select(['app_id', 'age_frac_8_10_with_null'])"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features with incomes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_income_features_add_table = train.select(['app_id']).dropDuplicates()\n",
        "\n",
        "train_income = train.filter(sf.col('income_flag') == 0)\n",
        "train_income = train_income.select(['app_id', 'amnt', 'days_before'])\n",
        "train_income = train_income.withColumn('more_5k', (sf.col('amnt') >= 5000).cast(IntegerType()))\n",
        "train_income = train_income.withColumn('more_15k', (sf.col('amnt') > 15000).cast(IntegerType()))\n",
        "train_income = train_income.groupBy(['app_id']).agg(sf.mean('more_5k').alias('add_income_perc_more_5k'),\n",
        "                                                    sf.sum('more_15k').alias('add_income_sum_more_15k'),\n",
        "                                                    sf.max('days_before').alias('add_income_max_days_before'))\n",
        "train_income = train_income.withColumn('add_income_per_month_income_more_15k',\n",
        "                        sf.col('add_income_sum_more_15k') / sf.col('add_income_max_days_before'))\n",
        "train_income = train_income.select(['app_id', 'add_income_perc_more_5k', 'add_income_per_month_income_more_15k'])\n",
        "train_income_features_add_table = train_income_features_add_table.join(train_income, how='left', on=['app_id'])\n",
        "\n",
        "train_last_month = train.filter(sf.col('income_flag') == 0).filter(sf.col('days_before') <= 31)\n",
        "train_last_month = train_last_month.groupBy(['app_id']).agg(sf.sum('amnt').alias('last_month_income_sum'))\n",
        "train_last_month = train_last_month.fillna(0, subset=['last_month_income_sum'])\n",
        "train_income_features_add_table = train_income_features_add_table.join(train_last_month, how='left', on=['app_id'])\n",
        "\n",
        "train_month_before_c = train.filter(sf.col('income_flag') == 0)\n",
        "train_month_before_c = train_month_before_c.select(['app_id', 'amnt', 'days_before'])\n",
        "train_month_before_c = train_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id', 'month_before']).agg(sf.sum('amnt').alias('amnt'))\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id']).agg(\n",
        "                                sf.mean('amnt').alias('add_income_month_income_debet_mean'),\n",
        "                                sf.count('amnt').alias('add_income_month_income_debet_cnt'),\n",
        "                                sf.max('amnt').alias('add_income_month_income_debet_max'),\n",
        "                                sf.stddev('amnt').alias('add_income_month_income_debet_std'),\n",
        "                                sf.min('month_before').alias('add_income_month_before_debet_min'))\n",
        "train_income_features_add_table = train_income_features_add_table.join(train_month_before_c,\n",
        "                                                                       how='left', on=['app_id'])\n",
        "train_month_before_c = train.filter(sf.col('income_flag') == 1)\n",
        "train_month_before_c = train_month_before_c.select(['app_id', 'amnt', 'days_before'])\n",
        "train_month_before_c = train_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id', 'month_before']).agg(sf.sum('amnt').alias('amnt'))\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id']).agg(\n",
        "                                sf.mean('amnt').alias('add_income_month_income_credit_mean'),\n",
        "                                sf.count('amnt').alias('add_income_month_income_credit_cnt'),\n",
        "                                sf.max('amnt').alias('add_income_month_income_credit_max'),\n",
        "                                sf.stddev('amnt').alias('add_income_month_income_credit_std'),\n",
        "                                sf.min('month_before').alias('add_income_month_before_credit_min'))\n",
        "train_income_features_add_table = train_income_features_add_table.join(train_month_before_c,\n",
        "                                                                       how='left', on=['app_id'])\n",
        "train_month_before_c = train.filter(sf.col('income_flag') == 0)\n",
        "train_month_before_c = train_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "train_month_before_c = train_month_before_c.withColumn('more_5k', (sf.col('amnt') >= 5000).cast(IntegerType()))\n",
        "train_month_before_c = train_month_before_c.withColumn('sum_more_5k', sf.col('more_5k') * sf.col('amnt'))\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id', 'month_before']).agg(\n",
        "                                                      sf.count('amnt').alias('cnt_income'),\n",
        "                                                      sf.sum('more_5k').alias('cnt_more_5k'),\n",
        "                                                      sf.mean('more_5k').alias('perc_more_5k'),\n",
        "                                                      sf.sum('sum_more_5k').alias('zp'))\n",
        "train_month_before_c = train_month_before_c.filter(sf.col('cnt_more_5k') > 0)\n",
        "train_month_before_c = train_month_before_c.groupBy(['app_id']).agg(\n",
        "                                      sf.min('month_before').alias('add_2222_month_before_min'),\n",
        "                                      sf.mean('cnt_income').alias('add_2222_cnt_income_mean'),\n",
        "                                      sf.stddev('cnt_income').alias('add_2222_cnt_income_std'),\n",
        "                                      sf.mean('cnt_more_5k').alias('add_2222_cnt_more_5k_mean'),\n",
        "                                      sf.stddev('cnt_more_5k').alias('add_2222_cnt_more_5k_std'),\n",
        "                                      sf.mean('perc_more_5k').alias('add_2222_perc_more_5k_mean'),\n",
        "                                      sf.stddev('perc_more_5k').alias('add_2222_perc_more_5k_std'),\n",
        "                                      sf.mean('zp').alias('add_2222_zp_mean'),\n",
        "                                      sf.stddev('zp').alias('add_2222_zp_std'))\n",
        "train_income_features_add_table = train_income_features_add_table.join(train_month_before_c,\n",
        "                                                                       how='left', on=['app_id'])"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_income_features_add_table = test.select(['app_id']).dropDuplicates()\n",
        "\n",
        "test_income = test.filter(sf.col('income_flag') == 0)\n",
        "test_income = test_income.select(['app_id', 'amnt', 'days_before'])\n",
        "test_income = test_income.withColumn('more_5k', (sf.col('amnt') >= 5000).cast(IntegerType()))\n",
        "test_income = test_income.withColumn('more_15k', (sf.col('amnt') > 15000).cast(IntegerType()))\n",
        "test_income = test_income.groupBy(['app_id']).agg(sf.mean('more_5k').alias('add_income_perc_more_5k'),\n",
        "                                                    sf.sum('more_15k').alias('add_income_sum_more_15k'),\n",
        "                                                    sf.max('days_before').alias('add_income_max_days_before'))\n",
        "test_income = test_income.withColumn('add_income_per_month_income_more_15k',\n",
        "                        sf.col('add_income_sum_more_15k') / sf.col('add_income_max_days_before'))\n",
        "test_income = test_income.select(['app_id', 'add_income_perc_more_5k', 'add_income_per_month_income_more_15k'])\n",
        "test_income_features_add_table = test_income_features_add_table.join(test_income, how='left', on=['app_id'])\n",
        "\n",
        "test_last_month = test.filter(sf.col('income_flag') == 0).filter(sf.col('days_before') <= 31)\n",
        "test_last_month = test_last_month.groupBy(['app_id']).agg(sf.sum('amnt').alias('last_month_income_sum'))\n",
        "test_last_month = test_last_month.fillna(0, subset=['last_month_income_sum'])\n",
        "test_income_features_add_table = test_income_features_add_table.join(test_last_month, how='left', on=['app_id'])\n",
        "\n",
        "test_month_before_c = test.filter(sf.col('income_flag') == 0)\n",
        "test_month_before_c = test_month_before_c.select(['app_id', 'amnt', 'days_before'])\n",
        "test_month_before_c = test_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id', 'month_before']).agg(sf.sum('amnt').alias('amnt'))\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id']).agg(\n",
        "                                sf.mean('amnt').alias('add_income_month_income_debet_mean'),\n",
        "                                sf.count('amnt').alias('add_income_month_income_debet_cnt'),\n",
        "                                sf.max('amnt').alias('add_income_month_income_debet_max'),\n",
        "                                sf.stddev('amnt').alias('add_income_month_income_debet_std'),\n",
        "                                sf.min('month_before').alias('add_income_month_before_debet_min'))\n",
        "test_income_features_add_table = test_income_features_add_table.join(test_month_before_c, how='left', on=['app_id'])\n",
        "\n",
        "test_month_before_c = test.filter(sf.col('income_flag') == 1)\n",
        "test_month_before_c = test_month_before_c.select(['app_id', 'amnt', 'days_before'])\n",
        "test_month_before_c = test_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id', 'month_before']).agg(sf.sum('amnt').alias('amnt'))\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id']).agg(\n",
        "                                sf.mean('amnt').alias('add_income_month_income_credit_mean'),\n",
        "                                sf.count('amnt').alias('add_income_month_income_credit_cnt'),\n",
        "                                sf.max('amnt').alias('add_income_month_income_credit_max'),\n",
        "                                sf.stddev('amnt').alias('add_income_month_income_credit_std'),\n",
        "                                sf.min('month_before').alias('add_income_month_before_credit_min'))\n",
        "test_income_features_add_table = test_income_features_add_table.join(test_month_before_c, how='left', on=['app_id'])\n",
        "\n",
        "test_month_before_c = test.filter(sf.col('income_flag') == 0)\n",
        "test_month_before_c = test_month_before_c.withColumn('month_before', sf.floor(sf.col('days_before') / sf.lit(30)))\n",
        "test_month_before_c = test_month_before_c.withColumn('more_5k', (sf.col('amnt') >= 5000).cast(IntegerType()))\n",
        "test_month_before_c = test_month_before_c.withColumn('sum_more_5k', sf.col('more_5k') * sf.col('amnt'))\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id', 'month_before']).agg(\n",
        "                                                      sf.count('amnt').alias('cnt_income'),\n",
        "                                                      sf.sum('more_5k').alias('cnt_more_5k'),\n",
        "                                                      sf.mean('more_5k').alias('perc_more_5k'),\n",
        "                                                      sf.sum('sum_more_5k').alias('zp'))\n",
        "test_month_before_c = test_month_before_c.filter(sf.col('cnt_more_5k') > 0)\n",
        "test_month_before_c = test_month_before_c.groupBy(['app_id']).agg(\n",
        "                                      sf.min('month_before').alias('add_2222_month_before_min'),\n",
        "                                      sf.mean('cnt_income').alias('add_2222_cnt_income_mean'),\n",
        "                                      sf.stddev('cnt_income').alias('add_2222_cnt_income_std'),\n",
        "                                      sf.mean('cnt_more_5k').alias('add_2222_cnt_more_5k_mean'),\n",
        "                                      sf.stddev('cnt_more_5k').alias('add_2222_cnt_more_5k_std'),\n",
        "                                      sf.mean('perc_more_5k').alias('add_2222_perc_more_5k_mean'),\n",
        "                                      sf.stddev('perc_more_5k').alias('add_2222_perc_more_5k_std'),\n",
        "                                      sf.mean('zp').alias('add_2222_zp_mean'),\n",
        "                                      sf.stddev('zp').alias('add_2222_zp_std'))\n",
        "test_income_features_add_table = test_income_features_add_table.join(test_month_before_c, how='left', on=['app_id'])"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cat cols balances"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target_for_balances = spark.table('alfa.andrey_auto_target_train')\n",
        "target_for_balances = target_for_balances.withColumnRenamed('flag', 'target')\n",
        "target_for_balances = target_for_balances.select(['app_id', 'target'])"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_balances_table = train.select(['app_id']).dropDuplicates()\n",
        "test_balances_table = test.select(['app_id']).dropDuplicates()\n",
        "    \n",
        "for c_col in ['currency', 'card_type', 'hour']:\n",
        "    current_cat_balance = train.select(['app_id', c_col])\n",
        "    current_cat_balance = current_cat_balance.join(target_for_balances, on='app_id', how='inner')\n",
        "    current_cat_balance = current_cat_balance.groupBy([c_col, 'target']).count()\n",
        "    current_cat_balance = current_cat_balance.groupBy(c_col).pivot('target').agg(sf.first('count'))\n",
        "    current_cat_balance = current_cat_balance.withColumnRenamed('0', 'good').withColumnRenamed('1', 'bad')\n",
        "    current_cat_balance = current_cat_balance.fillna(0, subset=['good', 'bad'])\n",
        "    current_cat_balance = current_cat_balance.withColumn('sum_trans', sf.col('bad') + sf.col('good'))\n",
        "    current_cat_balance = current_cat_balance.withColumn('balance', sf.col('bad') / sf.col('sum_trans'))\n",
        "    current_cat_balance = current_cat_balance.select([c_col, 'balance'])\n",
        "\n",
        "    if c_col in ['card_type', 'hour']:\n",
        "        f_table = train.select(['app_id', 'amnt', c_col])\n",
        "        f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "        f_table = f_table.groupBy(['app_id']).agg(sf.mean('balance').alias('mean_balance_' + c_col + '_cnt'))\n",
        "        train_balances_table = train_balances_table.join(f_table, on=['app_id'], how='left')\n",
        "        f_table = test.select(['app_id', 'amnt', c_col])\n",
        "        f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "        f_table = f_table.groupBy(['app_id']).agg(sf.mean('balance').alias('mean_balance_' + c_col + '_cnt'))\n",
        "        test_balances_table = test_balances_table.join(f_table, on=['app_id'], how='left')\n",
        "    \n",
        "    if c_col in ['currency']:\n",
        "        f_table = train.select(['app_id', 'amnt', c_col]).filter(sf.col('amnt') > 0)\n",
        "        f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "        f_table = f_table.withColumn('sum_amnt', sf.sum('amnt').over(sw().partitionBy('app_id')))\n",
        "        f_table = f_table.withColumn('fraq_amnt', sf.col('amnt') / sf.col('sum_amnt'))\n",
        "        f_table = f_table.withColumn('w_balance', sf.col('fraq_amnt') * sf.col('balance'))\n",
        "        f_table = f_table.groupBy(['app_id']).agg(sf.sum('w_balance').alias('w_mean_balance_' + c_col + '_cnt'))\n",
        "        train_balances_table = train_balances_table.join(f_table, on=['app_id'], how='left')\n",
        "        f_table = test.select(['app_id', 'amnt', c_col]).filter(sf.col('amnt') > 0)\n",
        "        f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "        f_table = f_table.withColumn('sum_amnt', sf.sum('amnt').over(sw().partitionBy('app_id')))\n",
        "        f_table = f_table.withColumn('fraq_amnt', sf.col('amnt') / sf.col('sum_amnt'))\n",
        "        f_table = f_table.withColumn('w_balance', sf.col('fraq_amnt') * sf.col('balance'))\n",
        "        f_table = f_table.groupBy(['app_id']).agg(sf.sum('w_balance').alias('w_mean_balance_' + c_col + '_cnt'))\n",
        "        test_balances_table = test_balances_table.join(f_table, on=['app_id'], how='left')\n",
        "\n",
        "for c_col in ['operation_kind', 'operation_type', 'mcc', 'mcc_category']:\n",
        "    current_cat_balance = train.select(['app_id', 'amnt', c_col]).filter(sf.col('amnt') > 0)\n",
        "    current_cat_balance = current_cat_balance.join(target_for_balances, on='app_id', how='inner')\n",
        "    current_cat_balance = current_cat_balance.groupBy([c_col, 'target']).agg(sf.sum('amnt').alias('count'))\n",
        "    current_cat_balance = current_cat_balance.groupBy(c_col).pivot('target').agg(sf.first('count'))\n",
        "    current_cat_balance = current_cat_balance.withColumnRenamed('0', 'good').withColumnRenamed('1', 'bad')\n",
        "    current_cat_balance = current_cat_balance.fillna(0, subset=['good', 'bad'])\n",
        "    current_cat_balance = current_cat_balance.withColumn('sum_trans', sf.col('bad') + sf.col('good'))\n",
        "    current_cat_balance = current_cat_balance.withColumn('balance', sf.col('bad') / sf.col('sum_trans'))\n",
        "    current_cat_balance = current_cat_balance.select([c_col, 'balance'])\n",
        "    \n",
        "    f_table = train.select(['app_id', 'amnt', c_col]).filter(sf.col('amnt') > 0)\n",
        "    f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "    f_table = f_table.withColumn('sum_amnt', sf.sum('amnt').over(sw().partitionBy('app_id')))\n",
        "    f_table = f_table.withColumn('fraq_amnt', sf.col('amnt') / sf.col('sum_amnt'))\n",
        "    f_table = f_table.withColumn('w_balance', sf.col('fraq_amnt') * sf.col('balance'))\n",
        "    f_table = f_table.groupBy(['app_id']).agg(sf.sum('w_balance').alias('w_mean_balance_' + c_col + '_amnt'))\n",
        "    train_balances_table = train_balances_table.join(f_table, on=['app_id'], how='left')   \n",
        "    f_table = test.select(['app_id', 'amnt', c_col]).filter(sf.col('amnt') > 0)\n",
        "    f_table = f_table.join(current_cat_balance, on=[c_col], how='inner')\n",
        "    f_table = f_table.withColumn('sum_amnt', sf.sum('amnt').over(sw().partitionBy('app_id')))\n",
        "    f_table = f_table.withColumn('fraq_amnt', sf.col('amnt') / sf.col('sum_amnt'))\n",
        "    f_table = f_table.withColumn('w_balance', sf.col('fraq_amnt') * sf.col('balance'))\n",
        "    f_table = f_table.groupBy(['app_id']).agg(sf.sum('w_balance').alias('w_mean_balance_' + c_col + '_amnt'))\n",
        "    test_balances_table = test_balances_table.join(f_table, on=['app_id'], how='left')"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bad, good features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_good_bad_table = train.select(['app_id']).dropDuplicates()\n",
        "for c_col in ['country_group', 'mcc_category_group']:\n",
        "    c_df = train.filter(sf.col('amnt') > 0)\n",
        "    c_df = c_df.groupBy(['app_id', c_col]).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "    c_df = c_df.withColumn('sum_all_' + c_col, sf.sum('sum_amnt').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq', sf.col('sum_amnt') / sf.col('sum_all_' + c_col))\n",
        "    c_df = c_df.groupBy(['app_id']).pivot(c_col).agg(sf.first('fraq'))\n",
        "    for col in c_df.columns:\n",
        "        if col != 'app_id':\n",
        "            c_df = c_df.fillna(0, subset=[col]).withColumnRenamed(col, 'fraq_sum_' + c_col + '_' + col)\n",
        "    c_df = c_df.select('app_id', 'fraq_sum_' + c_col + '_bad', 'fraq_sum_' + c_col + '_good')\n",
        "    train_good_bad_table = train_good_bad_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_good_bad_table = test.select(['app_id']).dropDuplicates()\n",
        "for c_col in ['country_group', 'mcc_category_group']:\n",
        "    c_df = test.filter(sf.col('amnt') > 0)\n",
        "    c_df = c_df.groupBy(['app_id', c_col]).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "    c_df = c_df.withColumn('sum_all_' + c_col, sf.sum('sum_amnt').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq', sf.col('sum_amnt') / sf.col('sum_all_' + c_col))\n",
        "    c_df = c_df.groupBy(['app_id']).pivot(c_col).agg(sf.first('fraq'))\n",
        "    for col in c_df.columns:\n",
        "        if col != 'app_id':\n",
        "            c_df = c_df.fillna(0, subset=[col]).withColumnRenamed(col, 'fraq_sum_' + c_col + '_' + col)\n",
        "    c_df = c_df.select('app_id', 'fraq_sum_' + c_col + '_bad', 'fraq_sum_' + c_col + '_good')\n",
        "    test_good_bad_table = test_good_bad_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Ex of columns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_Ex_table = train.select(['app_id']).dropDuplicates()\n",
        "for c_col in ['is_night_trans', 'rub_trans', 'income_flag', 'is_weekend_trans_67', 'is_weekend_trans_12']:\n",
        "    c_df = train.filter(sf.col('amnt') > 0)\n",
        "    c_df = c_df.groupBy(['app_id', c_col]).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "    c_df = c_df.withColumn('sum_all', sf.sum('sum_amnt').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq_sum_' + c_col, sf.col('sum_amnt') / sf.col('sum_all'))\n",
        "    c_df = c_df.groupBy('app_id').pivot(c_col).agg(sf.first('fraq_sum_' + c_col))\n",
        "    c_df = c_df.fillna(0, subset=['1']).withColumnRenamed('1', 'fraq_sum_' + c_col)\n",
        "    c_df = c_df.select(['app_id', 'fraq_sum_' + c_col])\n",
        "    train_Ex_table = train_Ex_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_Ex_table = test.select(['app_id']).dropDuplicates()\n",
        "for c_col in ['is_night_trans', 'rub_trans', 'income_flag', 'is_weekend_trans_67', 'is_weekend_trans_12']:\n",
        "    c_df = test.filter(sf.col('amnt') > 0)\n",
        "    c_df = c_df.groupBy(['app_id', c_col]).agg(sf.sum('amnt').alias('sum_amnt'))\n",
        "    c_df = c_df.withColumn('sum_all', sf.sum('sum_amnt').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq_sum_' + c_col, sf.col('sum_amnt') / sf.col('sum_all'))\n",
        "    c_df = c_df.groupBy('app_id').pivot(c_col).agg(sf.first('fraq_sum_' + c_col))\n",
        "    c_df = c_df.fillna(0, subset=['1']).withColumnRenamed('1', 'fraq_sum_' + c_col)\n",
        "    c_df = c_df.select(['app_id', 'fraq_sum_' + c_col])\n",
        "    test_Ex_table = test_Ex_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add mode for columns and statistics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_mode_table = train.select(['app_id']).dropDuplicates()\n",
        "for_mmode_cols = ['currency', 'card_type', 'operation_type', 'operation_type_group',\n",
        "                  'mcc', 'city', 'mcc_category', 'day_of_week', 'hour', 'n_order']\n",
        "for c_col in for_mmode_cols:\n",
        "    c_df = train.groupBy(['app_id', c_col]).count().alias('counts')\n",
        "    c_df = c_df.withColumn('cnt', sf.sum('count').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('cnt_fraction_' + c_col, sf.col('count') / sf.col('cnt'))\n",
        "    c_df = c_df.withColumn('std_fraction_' + c_col,\n",
        "                           sf.stddev('cnt_fraction_' + c_col).over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('cnt_unique_' + c_col, sf.count('cnt_fraction_' + c_col).over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('row_n', sf.row_number().over(sw.partitionBy(['app_id'])\\\n",
        "                            .orderBy(sf.desc('cnt_fraction_' + c_col)))).where('row_n=1').drop('row_n')\n",
        "    c_df = c_df.withColumnRenamed(c_col, c_col + '_mode')\n",
        "    c_df = c_df.select(['app_id', c_col + '_mode', 'std_fraction_' + c_col,\n",
        "                        'cnt_unique_' + c_col, 'cnt_fraction_' + c_col])\n",
        "    train_mode_table = train_mode_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_mode_table = test.select(['app_id']).dropDuplicates()\n",
        "for_mmode_cols = ['currency', 'card_type', 'operation_type', 'operation_type_group',\n",
        "                  'mcc', 'city', 'mcc_category', 'day_of_week', 'hour', 'n_order']\n",
        "for c_col in for_mmode_cols:\n",
        "    c_df = test.groupBy(['app_id', c_col]).count().alias('counts')\n",
        "    c_df = c_df.withColumn('cnt', sf.sum('count').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('cnt_fraction_' + c_col, sf.col('count') / sf.col('cnt'))\n",
        "    c_df = c_df.withColumn('std_fraction_' + c_col,\n",
        "                           sf.stddev('cnt_fraction_' + c_col).over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('cnt_unique_' + c_col, sf.count('cnt_fraction_' + c_col).over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('row_n', sf.row_number().over(sw.partitionBy(['app_id'])\\\n",
        "                            .orderBy(sf.desc('cnt_fraction_' + c_col)))).where('row_n=1').drop('row_n')\n",
        "    c_df = c_df.withColumnRenamed(c_col, c_col + '_mode')\n",
        "    c_df = c_df.select(['app_id', c_col + '_mode', 'std_fraction_' + c_col,\n",
        "                        'cnt_unique_' + c_col, 'cnt_fraction_' + c_col])             \n",
        "    test_mode_table = test_mode_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add cols counts"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_cats_table = train.select(['app_id']).dropDuplicates()\n",
        "for_cat_cols = ['currency', 'operation_kind', 'day_of_week', 'operation_type', 'mcc_category',\n",
        "                'mcc', 'card_type', 'hour', 'operation_type_group']#, 'city']\n",
        "for c_col in for_cat_cols:\n",
        "    c_df = train.groupBy(['app_id', c_col]).count().alias('counts')\n",
        "    c_df = c_df.withColumn('sum_' + c_col, sf.sum('count').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq', sf.col('count') / sf.col('sum_' + c_col))\n",
        "    c_df = c_df.groupBy(['app_id']).pivot(c_col).agg(sf.first('fraq'))\n",
        "    for col in c_df.columns:\n",
        "        if col != 'app_id':\n",
        "            c_df = c_df.fillna(0, subset=[col]).withColumnRenamed(col, 'fraq_' + c_col + '_' + col)\n",
        "    train_cats_table = train_cats_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_cats_table = test.select(['app_id']).dropDuplicates()\n",
        "for_cat_cols = ['currency', 'operation_kind', 'day_of_week', 'operation_type', 'mcc_category',\n",
        "                'mcc', 'card_type', 'hour', 'operation_type_group']#, 'city']\n",
        "for c_col in for_cat_cols:\n",
        "    c_df = test.groupBy(['app_id', c_col]).count().alias('counts')\n",
        "    c_df = c_df.withColumn('sum_' + c_col, sf.sum('count').over(sw().partitionBy('app_id')))\n",
        "    c_df = c_df.withColumn('fraq', sf.col('count') / sf.col('sum_' + c_col))\n",
        "    c_df = c_df.groupBy(['app_id']).pivot(c_col).agg(sf.first('fraq'))\n",
        "    for col in c_df.columns:\n",
        "        if col != 'app_id':\n",
        "            c_df = c_df.fillna(0, subset=[col]).withColumnRenamed(col, 'fraq_' + c_col + '_' + col)\n",
        "    test_cats_table = test_cats_table.join(c_df, ['app_id'], 'left')"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agg with zero statistics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "agg_non_0_cols = ['app_id', 'amnt', 'days_before', 'hour_diff', 'is_night_trans',\n",
        "                  'ecommerce_flag', 'income_flag']\n",
        "train_agg_with_zero = train.select(agg_non_0_cols)\n",
        "\n",
        "train_agg_with_zero = train_agg_with_zero.withColumn(\"days_before_pv\", sf.lag(train_agg_with_zero.days_before)\\\n",
        "                             .over(sw.partitionBy(['app_id']).orderBy(sf.desc('days_before'))))\n",
        "train_agg_with_zero = train_agg_with_zero.filter(sf.col('days_before_pv').isNotNull())\n",
        "train_agg_with_zero = train_agg_with_zero.withColumn(\"diff_days\",\n",
        "                            train_agg_with_zero.days_before_pv - train_agg_with_zero.days_before)\n",
        "\n",
        "train_agg_with_zero = train_agg_with_zero.groupBy(['app_id']).agg(\n",
        "                                    sf.max('diff_days').alias('max_days_without_trans'),\n",
        "                                    sf.mean('diff_days').alias('mean_days_diff'),\n",
        "                                    sf.stddev('diff_days').alias('std_diff_days'),\n",
        "\n",
        "                                    sf.max('hour_diff').alias('max_hours_without_trans'),\n",
        "                                    sf.expr('percentile_approx(hour_diff, 0.5)').alias('median_hour_diff'),\n",
        "                                    sf.stddev('hour_diff').alias('std_hour_diff'),\n",
        "\n",
        "                                    sf.mean('is_night_trans').alias('mean_is_night_trans'),\n",
        "                                    sf.mean('ecommerce_flag').alias('mean_ecommerce_flag'),\n",
        "                                    sf.mean('income_flag').alias('mean_income_flag'),\n",
        "                                    sf.max('days_before').alias('days_from_first_transaction'))"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "agg_non_0_cols = ['app_id', 'amnt', 'days_before', 'hour_diff', 'is_night_trans',\n",
        "                  'ecommerce_flag', 'income_flag']\n",
        "test_agg_with_zero = test.select(agg_non_0_cols)\n",
        "\n",
        "test_agg_with_zero = test_agg_with_zero.withColumn(\"days_before_pv\", sf.lag(test_agg_with_zero.days_before)\\\n",
        "                             .over(sw.partitionBy(['app_id']).orderBy(sf.desc('days_before'))))\n",
        "test_agg_with_zero = test_agg_with_zero.filter(sf.col('days_before_pv').isNotNull())\n",
        "test_agg_with_zero = test_agg_with_zero.withColumn(\"diff_days\",\n",
        "                            test_agg_with_zero.days_before_pv - test_agg_with_zero.days_before)\n",
        "\n",
        "test_agg_with_zero = test_agg_with_zero.groupBy(['app_id']).agg(\n",
        "                                    sf.max('diff_days').alias('max_days_without_trans'),\n",
        "                                    sf.mean('diff_days').alias('mean_days_diff'),\n",
        "                                    sf.stddev('diff_days').alias('std_diff_days'),\n",
        "\n",
        "                                    sf.max('hour_diff').alias('max_hours_without_trans'),\n",
        "                                    sf.expr('percentile_approx(hour_diff, 0.5)').alias('median_hour_diff'),\n",
        "                                    sf.stddev('hour_diff').alias('std_hour_diff'),\n",
        "\n",
        "                                    sf.mean('is_night_trans').alias('mean_is_night_trans'),\n",
        "                                    sf.mean('ecommerce_flag').alias('mean_ecommerce_flag'),\n",
        "                                    sf.mean('income_flag').alias('mean_income_flag'),\n",
        "                                    sf.max('days_before').alias('days_from_first_transaction'))"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agg non zero statistics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "agg_non_0_cols = ['app_id', 'amnt', 'day_of_week', 'days_before', 'ecommerce_flag', 'hour', 'hour_diff',\n",
        "                  'income_flag', 'transaction_number', 'weekofyear']\n",
        "train_agg_non_zero = train.select(agg_non_0_cols).filter(sf.col('amnt') > 0)\n",
        "\n",
        "train_agg_non_zero = train_agg_non_zero.groupBy(['app_id']).agg(\n",
        "                                    sf.mean('amnt').alias('mean_amnt'),\n",
        "                                    sf.stddev('amnt').alias('std_amnt'),\n",
        "                                    sf.max('amnt').alias('max_amnt'),\n",
        "                                    sf.expr('percentile_approx(amnt, 0.5)').alias('median_amnt'))"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "agg_non_0_cols = ['app_id', 'amnt', 'day_of_week', 'days_before', 'ecommerce_flag', 'hour', 'hour_diff',\n",
        "                  'income_flag', 'transaction_number', 'weekofyear']\n",
        "test_agg_non_zero = test.select(agg_non_0_cols).filter(sf.col('amnt') > 0)\n",
        "\n",
        "test_agg_non_zero = test_agg_non_zero.groupBy(['app_id']).agg(\n",
        "                                    sf.mean('amnt').alias('mean_amnt'),\n",
        "                                    sf.stddev('amnt').alias('std_amnt'),\n",
        "                                    sf.max('amnt').alias('max_amnt'),\n",
        "                                    sf.expr('percentile_approx(amnt, 0.5)').alias('median_amnt'))"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add last day transaction features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_last_trans = train.withColumn('min_days_before', sf.min('days_before').over(sw().partitionBy('app_id')))\n",
        "train_last_trans = train_last_trans.filter(sf.col('days_before') == sf.col('min_days_before'))\n",
        "\n",
        "train_last_trans = train_last_trans.withColumn('cnt_last_day', sf.count('amnt').over(sw().partitionBy('app_id')))\n",
        "train_last_trans = train_last_trans.withColumn('min_hour', sf.min('hour').over(sw().partitionBy('app_id')))\n",
        "train_last_trans = train_last_trans.filter(sf.col('hour') == sf.col('min_hour'))\n",
        "\n",
        "train_last_trans = train_last_trans.withColumn('cnt_last_hour', sf.count('amnt').over(sw().partitionBy('app_id')))\n",
        "train_last_trans = train_last_trans.withColumn('max_last_day_hours_diff',\n",
        "                                               sf.max('hour_diff').over(sw().partitionBy('app_id')))\n",
        "\n",
        "train_last_trans = train_last_trans.withColumn('last_transaction_number',\n",
        "                                               sf.max('transaction_number').over(sw().partitionBy('app_id')))\n",
        "train_last_trans = train_last_trans.filter(sf.col('transaction_number') == sf.col('last_transaction_number'))\n",
        "\n",
        "select_last_trans_cols = ['app_id', 'mcc', 'days_before']\n",
        "train_last_trans = train_last_trans.select(select_last_trans_cols)\n",
        "for col in ['mcc', 'days_before', 'weekofyear', 'min_days_before']:\n",
        "    train_last_trans = train_last_trans.withColumnRenamed(col, 'last_day_' + col)"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_last_trans = test.withColumn('min_days_before', sf.min('days_before').over(sw().partitionBy('app_id')))\n",
        "test_last_trans = test_last_trans.filter(sf.col('days_before') == sf.col('min_days_before'))\n",
        "\n",
        "test_last_trans = test_last_trans.withColumn('cnt_last_day', sf.count('amnt').over(sw().partitionBy('app_id')))\n",
        "test_last_trans = test_last_trans.withColumn('min_hour', sf.min('hour').over(sw().partitionBy('app_id')))\n",
        "test_last_trans = test_last_trans.filter(sf.col('hour') == sf.col('min_hour'))\n",
        "\n",
        "test_last_trans = test_last_trans.withColumn('cnt_last_hour', sf.count('amnt').over(sw().partitionBy('app_id')))\n",
        "test_last_trans = test_last_trans.withColumn('max_last_day_hours_diff',\n",
        "                                             sf.max('hour_diff').over(sw().partitionBy('app_id')))\n",
        "\n",
        "test_last_trans = test_last_trans.withColumn('last_transaction_number',\n",
        "                                             sf.max('transaction_number').over(sw().partitionBy('app_id')))\n",
        "test_last_trans = test_last_trans.filter(sf.col('transaction_number') == sf.col('last_transaction_number'))\n",
        "\n",
        "select_last_trans_cols = ['app_id', 'mcc', 'days_before']\n",
        "test_last_trans = test_last_trans.select(select_last_trans_cols)\n",
        "\n",
        "for col in ['mcc', 'days_before', 'weekofyear', 'min_days_before']:\n",
        "    test_last_trans = test_last_trans.withColumnRenamed(col, 'last_day_' + col)"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge features and Add target"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target_train = spark.table('alfa.andrey_auto_target_train')\n",
        "target_train = target_train.withColumnRenamed('flag', 'target')\n",
        "\n",
        "target_train = target_train.join(train_income_features_add_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_number_order, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_hours_diff_features, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_balances_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_income_flag_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_age_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_good_bad_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_Ex_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_mode_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_cats_table, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_agg_with_zero, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_agg_non_zero, on=['app_id'], how='left')\n",
        "target_train = target_train.join(train_last_trans, on=['app_id'], how='left')"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target_test = spark.table('alfa.andrey_auto_target_test')\n",
        "target_test = target_test.withColumn('target', sf.lit(-1))\n",
        "\n",
        "target_test = target_test.join(test_income_features_add_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_number_order, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_hours_diff_features, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_balances_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_income_flag_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_age_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_good_bad_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_Ex_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_mode_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_cats_table, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_agg_with_zero, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_agg_non_zero, on=['app_id'], how='left')\n",
        "target_test = target_test.join(test_last_trans, on=['app_id'], how='left')"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features after merge"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_train.columns), len(target_test.columns)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "(505, 507)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_train.columns), len(target_test.columns)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "(514, 516)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save tables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "target_train.write.format('orc').mode('overwrite').saveAsTable('alfa.andrey_auto_payments_targets_train')\n",
        "spark.table('alfa.andrey_auto_payments_targets_train').toPandas().to_pickle('./train.pkl')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7min 37s, sys: 48.7 s, total: 8min 25s\n",
            "Wall time: 22min 45s\n"
          ]
        }
      ],
      "execution_count": 43,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "target_test.write.format('orc').mode('overwrite').saveAsTable('alfa.andrey_auto_payments_targets_test')\n",
        "spark.table('alfa.andrey_auto_payments_targets_test').toPandas().to_pickle('./test.pkl')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 43s, sys: 18.3 s, total: 4min 2s\n",
            "Wall time: 14min 8s\n"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "______"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.27.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}